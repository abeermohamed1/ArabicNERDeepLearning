{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "testflair.ipynbT",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKq9n6i70o81",
        "outputId": "9b7e959c-b239-4051-c591-e75696135c20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "pip install --upgrade flair"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting flair\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/19/902d1691c1963ab8c9a9578abc2d65c63aa1ecf4f8200143b5ef91ace6f5/flair-0.6.1-py3-none-any.whl (331kB)\n",
            "\r\u001b[K     |█                               | 10kB 21.9MB/s eta 0:00:01\r\u001b[K     |██                              | 20kB 2.1MB/s eta 0:00:01\r\u001b[K     |███                             | 30kB 2.8MB/s eta 0:00:01\r\u001b[K     |████                            | 40kB 3.1MB/s eta 0:00:01\r\u001b[K     |█████                           | 51kB 2.5MB/s eta 0:00:01\r\u001b[K     |██████                          | 61kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████                         | 71kB 3.1MB/s eta 0:00:01\r\u001b[K     |████████                        | 81kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████                       | 92kB 3.7MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 102kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 112kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 122kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 133kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 143kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 153kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 163kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 174kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 184kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 194kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 204kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 215kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 225kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 235kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 245kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 256kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 266kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 276kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 286kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 296kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 307kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 317kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 327kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 337kB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from flair) (1.6.0+cu101)\n",
            "Collecting segtok>=1.5.7\n",
            "  Downloading https://files.pythonhosted.org/packages/41/08/582dab5f4b1d5ca23bc6927b4bb977c8ff7f3a87a3b98844ef833e2f5623/segtok-1.5.10.tar.gz\n",
            "Collecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/e2/3b51c53dffb1e52d9210ebc01f1fb9f2f6eba9b3201fa971fd3946643c71/ftfy-5.8.tar.gz (64kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 5.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: gensim>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from flair) (3.6.0)\n",
            "Collecting janome\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/63/98858cbead27df7536c7e300c169da0999e9704d02220dc6700b804eeff0/Janome-0.4.1-py2.py3-none-any.whl (19.7MB)\n",
            "\u001b[K     |████████████████████████████████| 19.7MB 1.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: gdown in /usr/local/lib/python3.6/dist-packages (from flair) (3.6.4)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from flair) (3.2.2)\n",
            "Collecting sqlitedict>=1.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/2d/b1d99e9ad157dd7de9cd0d36a8a5876b13b55e4b75f7498bc96035fb4e96/sqlitedict-1.7.0.tar.gz\n",
            "Collecting mpld3==0.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/95/a52d3a83d0a29ba0d6898f6727e9858fe7a43f6c2ce81a5fe7e05f0f4912/mpld3-0.3.tar.gz (788kB)\n",
            "\u001b[K     |████████████████████████████████| 798kB 53.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: tqdm>=4.26.0 in /usr/local/lib/python3.6/dist-packages (from flair) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn>=0.21.3 in /usr/local/lib/python3.6/dist-packages (from flair) (0.22.2.post1)\n",
            "Collecting deprecated>=1.2.4\n",
            "  Downloading https://files.pythonhosted.org/packages/76/a1/05d7f62f956d77b23a640efc650f80ce24483aa2f85a09c03fb64f49e879/Deprecated-1.2.10-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: lxml in /usr/local/lib/python3.6/dist-packages (from flair) (4.2.6)\n",
            "Collecting pytest>=5.3.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d6/36/9e022b76a3ac440e1d750c64fa6152469f988efe0c568b945e396e2693b5/pytest-6.1.1-py3-none-any.whl (272kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 49.3MB/s \n",
            "\u001b[?25hCollecting bpemb>=0.3.2\n",
            "  Downloading https://files.pythonhosted.org/packages/91/77/3f0f53856e86af32b1d3c86652815277f7b5f880002584eb30db115b6df5/bpemb-0.3.2-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: regex in /usr/local/lib/python3.6/dist-packages (from flair) (2019.12.20)\n",
            "Collecting langdetect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/a3/8407c1e62d5980188b4acc45ef3d94b933d14a2ebc9ef3505f22cf772570/langdetect-1.0.8.tar.gz (981kB)\n",
            "\u001b[K     |████████████████████████████████| 983kB 57.7MB/s \n",
            "\u001b[?25hCollecting konoha<5.0.0,>=4.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/ea/01/47358efec5396fc80f98273c42cbdfe7aab056252b07884ffcc0f118978f/konoha-4.6.2-py3-none-any.whl\n",
            "Collecting transformers>=3.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2c/4e/4f1ede0fd7a36278844a277f8d53c21f88f37f3754abf76a5d6224f76d4a/transformers-3.4.0-py3-none-any.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 57.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: tabulate in /usr/local/lib/python3.6/dist-packages (from flair) (0.8.7)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from flair) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: hyperopt>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from flair) (0.1.2)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 56.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.1.0->flair) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=1.1.0->flair) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->flair) (0.2.5)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair) (2.2.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from gdown->flair) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (1.2.0)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->flair) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: wrapt<2,>=1.10 in /usr/local/lib/python3.6/dist-packages (from deprecated>=1.2.4->flair) (1.12.1)\n",
            "Requirement already satisfied, skipping upgrade: py>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (1.9.0)\n",
            "Requirement already satisfied, skipping upgrade: toml in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (0.10.1)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata>=0.12; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (2.0.0)\n",
            "Collecting pluggy<1.0,>=0.12\n",
            "  Downloading https://files.pythonhosted.org/packages/a0/28/85c7aa31b80d150b772fbe4a229487bc6644da9ccb7e427dd8cc60cb8a62/pluggy-0.13.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: iniconfig in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (1.1.1)\n",
            "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (20.4)\n",
            "Requirement already satisfied, skipping upgrade: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (20.2.0)\n",
            "Collecting overrides==3.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/42/8d/caa729f809ecdf8e76fac3c1ff7d3f0b72c398c9dd8a6919927a30a873b3/overrides-3.0.0.tar.gz\n",
            "Collecting tokenizers==0.9.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/a5/78be1a55b2ac8d6a956f0a211d372726e2b1dd2666bb537fea9b03abd62c/tokenizers-0.9.2-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 63.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.0->flair) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.0->flair) (3.0.12)\n",
            "Requirement already satisfied, skipping upgrade: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.0->flair) (0.7)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 55.0MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (3.11.0)\n",
            "Requirement already satisfied, skipping upgrade: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (2.5)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gdown->flair) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gdown->flair) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gdown->flair) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gdown->flair) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest>=5.3.2->flair) (3.2.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers>=3.0.0->flair) (50.3.0)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=3.0.0->flair) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->hyperopt>=0.1.1->flair) (4.4.2)\n",
            "Building wheels for collected packages: segtok, ftfy, sqlitedict, mpld3, langdetect, overrides, sacremoses\n",
            "  Building wheel for segtok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segtok: filename=segtok-1.5.10-cp36-none-any.whl size=25021 sha256=e869a1dd0c415a6206d1fe00c2bf4b49e9cdfa078bf3ae94e95a0079b6a61e7d\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/39/f6/9ca1c5cabde964d728023b5751c3a206a5c8cc40252321fb6b\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-5.8-cp36-none-any.whl size=45612 sha256=d754f85a6dc72644cd4b9ed4f21b3185e14958a4f7e4f40d611d9b23e3cc7214\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/c0/ef/f28c4da5ac84a4e06ac256ca9182fc34fa57fefffdbc68425b\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-1.7.0-cp36-none-any.whl size=14377 sha256=c16faf5feadde77f68b9a7bce785e12568406bc79f1c5c3cf6d8cd670f172ef4\n",
            "  Stored in directory: /root/.cache/pip/wheels/cf/c6/4f/2c64a43f041415eb8b8740bd80e15e92f0d46c5e464d8e4b9b\n",
            "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpld3: filename=mpld3-0.3-cp36-none-any.whl size=116677 sha256=f6ef355eb42ff5f2ad06103973500e0e533f43f80ce1ad1b0eaf1d5f3bf96268\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/47/fb/8a64f89aecfe0059830479308ad42d62e898a3e3cefdf6ba28\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.8-cp36-none-any.whl size=993195 sha256=751205abd8974828b07ab2093fe2ac3861a48fff4eadc9dab7240d648ff2f2e1\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/b3/aa/6d99de9f3841d7d3d40a60ea06e6d669e8e5012e6c8b947a57\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.0.0-cp36-none-any.whl size=5669 sha256=b7bf74be00358bbafd40deb96b5b73580e0b8ccacb49c30f62b9c24c09a4fcf3\n",
            "  Stored in directory: /root/.cache/pip/wheels/6f/1b/ec/6c71a1eb823df7f850d956b2d8c50a6d49c191e1063d73b9be\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=90e8092312961ea7caee30c890a1fe127272a2bb4e4a2040cc091f7f63db0d2d\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built segtok ftfy sqlitedict mpld3 langdetect overrides sacremoses\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: segtok, ftfy, janome, sqlitedict, mpld3, deprecated, pluggy, pytest, sentencepiece, bpemb, langdetect, overrides, konoha, tokenizers, sacremoses, transformers, flair\n",
            "  Found existing installation: pluggy 0.7.1\n",
            "    Uninstalling pluggy-0.7.1:\n",
            "      Successfully uninstalled pluggy-0.7.1\n",
            "  Found existing installation: pytest 3.6.4\n",
            "    Uninstalling pytest-3.6.4:\n",
            "      Successfully uninstalled pytest-3.6.4\n",
            "Successfully installed bpemb-0.3.2 deprecated-1.2.10 flair-0.6.1 ftfy-5.8 janome-0.4.1 konoha-4.6.2 langdetect-1.0.8 mpld3-0.3 overrides-3.0.0 pluggy-0.13.1 pytest-6.1.1 sacremoses-0.0.43 segtok-1.5.10 sentencepiece-0.1.91 sqlitedict-1.7.0 tokenizers-0.9.2 transformers-3.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0L-4FnwAQ6C"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdSbj19T4T6l",
        "outputId": "70206143-ae17-454a-d5e3-7d31a926496f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/' ,force_remount=True)\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTnIPBR9UBdp",
        "outputId": "68b0a253-e5ef-45f9-fa79-2672be1b3d56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from flair.data import Corpus\n",
        "import gensim\n",
        "\n",
        "from flair.embeddings import FastTextEmbeddings,TokenEmbeddings, CharacterEmbeddings,  WordEmbeddings, StackedEmbeddings, PooledFlairEmbeddings,FlairEmbeddings,DocumentPoolEmbeddings\n",
        "from typing import List\n",
        "from flair.data import Dictionary\n",
        "from flair.models import LanguageModel\n",
        "from flair.trainers.language_model_trainer import LanguageModelTrainer, TextCorpus\n",
        "from flair.data import Corpus\n",
        "from flair.datasets import ColumnCorpus\n",
        "from flair.embeddings import BertEmbeddings\n",
        "import re\n",
        "import numpy as np\n",
        "from nltk import ngrams\n",
        "\n",
        "# define columns\n",
        "columns = {0: 'text', 1: 'ner'}\n",
        "\n",
        "# this is the folder in which train, test and dev files reside\n",
        "data_folder = '/content/gdrive/My Drive/resources/tasks/conll_03'\n",
        "\n",
        "\n",
        "# init a corpus using column format, data folder and the names of the train, dev and test files\n",
        "corpus: Corpus = ColumnCorpus(data_folder, columns,\n",
        "                              train_file='train.txt',\n",
        "                              test_file='test.txt',\n",
        "                              dev_file='dev.txt')\n",
        "    \n",
        "    \n",
        "\n",
        "# 2. what tag do we want to predict?\n",
        "tag_type = 'ner'\n",
        "\n",
        "# 3. make the tag dictionary from the corpus\n",
        "tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\n",
        "\n",
        "embedding_types: List[TokenEmbeddings] = [\n",
        "\n",
        "    # fastext embeddings\n",
        "    #WordEmbeddings(data_folder+'/model.bin',True),\n",
        "    #WordEmbeddings(data_folder+'/full_grams_cbow_300_wiki.mdl'),\n",
        "    #BertEmbeddings('bert-base-multilingual-cased'),\n",
        "    #WordEmbeddings( data_folder+'/modelsg300wiki.gensim', 'custom'),\n",
        "    #CharacterEmbeddings(path_to_char_dict=data_folder+'/pytorch_model.bin',True)\n",
        "    #BertEmbeddings(bert_model_or_path = data_folder+'/config.json', data_folder+'/pytorch_model.bin' ,  data_folder+'/vocab.txt'), \n",
        "    BertEmbeddings(bert_model_or_path = data_folder),\n",
        "    #flair fast embedding \n",
        "    WordEmbeddings('ar'),\n",
        "  \n",
        "\n",
        "    # orignal fast embeddings  from face\n",
        "    #FastTextEmbeddings(data_folder+'/fasttext300.bin'),\n",
        "    # BytePairEmbeddings('multi'),\n",
        "    # contextual string embeddings, forward\n",
        "    PooledFlairEmbeddings('ar-forward',  pooling= 'mean'),\n",
        "    # contextual string embeddings, backward\n",
        "    PooledFlairEmbeddings('ar-backward' ,  pooling= 'mean'),\n",
        "]\n",
        "\n",
        "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)\n",
        "\n",
        "#embeddings = DocumentPoolEmbeddings(embeddings=embedding_types)\n",
        "\n",
        "# initialize sequence tagger\n",
        "from flair.models import SequenceTagger\n",
        "\n",
        "tagger: SequenceTagger = SequenceTagger(hidden_size=256,\n",
        "                                        embeddings=embeddings,\n",
        "                                        tag_dictionary=tag_dictionary,\n",
        "                                        tag_type=tag_type  )\n",
        "\n",
        "# initialize trainer\n",
        "from flair.trainers import ModelTrainer\n",
        "from torch.optim.adam import Adam\n",
        "trainer: ModelTrainer = ModelTrainer(tagger, corpus)\n",
        "                                        \n",
        "                            \n",
        "import pickle\n",
        "\n",
        "trainer.train('/content/gdrive/My Drive/resources/taggers/example-ner',\n",
        "              train_with_dev=True, \n",
        "              learning_rate=0.1,   \n",
        "              mini_batch_size=16,\n",
        "              max_epochs=150,\n",
        "              checkpoint=True)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-10-22 16:15:24,977 Reading data from /content/gdrive/My Drive/resources/tasks/conll_03\n",
            "2020-10-22 16:15:24,977 Train: /content/gdrive/My Drive/resources/tasks/conll_03/train.txt\n",
            "2020-10-22 16:15:24,978 Dev: /content/gdrive/My Drive/resources/tasks/conll_03/dev.txt\n",
            "2020-10-22 16:15:24,979 Test: /content/gdrive/My Drive/resources/tasks/conll_03/test.txt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:46: DeprecationWarning: Call to deprecated method __init__. (Use 'TransformerWordEmbeddings' for all transformer-based word embeddings) -- Deprecated since version 0.4.5.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-10-22 16:15:39,897 https://flair.informatik.hu-berlin.de/resources/embeddings/token/ar-wiki-fasttext-300d-1M.vectors.npy not found in cache, downloading to /tmp/tmp6_vs5jes\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 733171328/733171328 [00:31<00:00, 23494770.10B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-10-22 16:16:11,535 copying /tmp/tmp6_vs5jes to cache at /root/.flair/embeddings/ar-wiki-fasttext-300d-1M.vectors.npy\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-10-22 16:16:13,826 removing temp file /tmp/tmp6_vs5jes\n",
            "2020-10-22 16:16:14,514 https://flair.informatik.hu-berlin.de/resources/embeddings/token/ar-wiki-fasttext-300d-1M not found in cache, downloading to /tmp/tmp51h4ox_q\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 26704903/26704903 [00:01<00:00, 13757053.05B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-10-22 16:16:16,872 copying /tmp/tmp51h4ox_q to cache at /root/.flair/embeddings/ar-wiki-fasttext-300d-1M\n",
            "2020-10-22 16:16:16,898 removing temp file /tmp/tmp51h4ox_q\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:252: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-10-22 16:16:20,277 https://flair.informatik.hu-berlin.de/resources/embeddings/flair/lm-ar-opus-large-forward-v0.1.pt not found in cache, downloading to /tmp/tmp6ctggpez\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 131796801/131796801 [00:13<00:00, 9550553.74B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-10-22 16:16:34,765 copying /tmp/tmp6ctggpez to cache at /root/.flair/embeddings/lm-ar-opus-large-forward-v0.1.pt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-10-22 16:16:34,897 removing temp file /tmp/tmp6ctggpez\n",
            "2020-10-22 16:16:51,000 https://flair.informatik.hu-berlin.de/resources/embeddings/flair/lm-ar-opus-large-backward-v0.1.pt not found in cache, downloading to /tmp/tmpvrlc3mo4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 131796811/131796811 [00:06<00:00, 21315076.40B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-10-22 16:16:57,712 copying /tmp/tmpvrlc3mo4 to cache at /root/.flair/embeddings/lm-ar-opus-large-backward-v0.1.pt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-10-22 16:16:57,836 removing temp file /tmp/tmpvrlc3mo4\n",
            "2020-10-22 16:17:02,187 ----------------------------------------------------------------------------------------------------\n",
            "2020-10-22 16:17:02,191 Model: \"SequenceTagger(\n",
            "  (embeddings): StackedEmbeddings(\n",
            "    (list_embedding_0): BertEmbeddings(\n",
            "      (model): BertModel(\n",
            "        (embeddings): BertEmbeddings(\n",
            "          (word_embeddings): Embedding(64000, 768, padding_idx=0)\n",
            "          (position_embeddings): Embedding(512, 768)\n",
            "          (token_type_embeddings): Embedding(2, 768)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (encoder): BertEncoder(\n",
            "          (layer): ModuleList(\n",
            "            (0): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (1): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (2): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (3): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (4): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (5): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (6): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (7): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (8): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (9): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (10): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (11): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (pooler): BertPooler(\n",
            "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (activation): Tanh()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (list_embedding_1): WordEmbeddings('ar')\n",
            "    (list_embedding_2): PooledFlairEmbeddings(\n",
            "      (context_embeddings): FlairEmbeddings(\n",
            "        (lm): LanguageModel(\n",
            "          (drop): Dropout(p=0.1, inplace=False)\n",
            "          (encoder): Embedding(7125, 100)\n",
            "          (rnn): LSTM(100, 2048)\n",
            "          (decoder): Linear(in_features=2048, out_features=7125, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (list_embedding_3): PooledFlairEmbeddings(\n",
            "      (context_embeddings): FlairEmbeddings(\n",
            "        (lm): LanguageModel(\n",
            "          (drop): Dropout(p=0.1, inplace=False)\n",
            "          (encoder): Embedding(7125, 100)\n",
            "          (rnn): LSTM(100, 2048)\n",
            "          (decoder): Linear(in_features=2048, out_features=7125, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (word_dropout): WordDropout(p=0.05)\n",
            "  (locked_dropout): LockedDropout(p=0.5)\n",
            "  (embedding2nn): Linear(in_features=11564, out_features=11564, bias=True)\n",
            "  (rnn): LSTM(11564, 256, batch_first=True, bidirectional=True)\n",
            "  (linear): Linear(in_features=512, out_features=12, bias=True)\n",
            "  (beta): 1.0\n",
            "  (weights): None\n",
            "  (weight_tensor) None\n",
            ")\"\n",
            "2020-10-22 16:17:02,193 ----------------------------------------------------------------------------------------------------\n",
            "2020-10-22 16:17:02,194 Corpus: \"Corpus: 1328 train + 710 dev + 605 test sentences\"\n",
            "2020-10-22 16:17:02,196 ----------------------------------------------------------------------------------------------------\n",
            "2020-10-22 16:17:02,200 Parameters:\n",
            "2020-10-22 16:17:02,203  - learning_rate: \"0.1\"\n",
            "2020-10-22 16:17:02,205  - mini_batch_size: \"16\"\n",
            "2020-10-22 16:17:02,206  - patience: \"3\"\n",
            "2020-10-22 16:17:02,207  - anneal_factor: \"0.5\"\n",
            "2020-10-22 16:17:02,208  - max_epochs: \"150\"\n",
            "2020-10-22 16:17:02,209  - shuffle: \"True\"\n",
            "2020-10-22 16:17:02,210  - train_with_dev: \"True\"\n",
            "2020-10-22 16:17:02,212  - batch_growth_annealing: \"False\"\n",
            "2020-10-22 16:17:02,213 ----------------------------------------------------------------------------------------------------\n",
            "2020-10-22 16:17:02,214 Model training base path: \"/content/gdrive/My Drive/resources/taggers/example-ner\"\n",
            "2020-10-22 16:17:02,215 ----------------------------------------------------------------------------------------------------\n",
            "2020-10-22 16:17:02,217 Device: cuda:0\n",
            "2020-10-22 16:17:02,218 ----------------------------------------------------------------------------------------------------\n",
            "2020-10-22 16:17:02,219 Embeddings storage mode: cpu\n",
            "2020-10-22 16:17:02,231 ----------------------------------------------------------------------------------------------------\n",
            "train mode resetting embeddings\n",
            "train mode resetting embeddings\n",
            "2020-10-22 16:17:14,208 epoch 1 - iter 12/128 - loss 26.96764922 - samples/sec: 16.14 - lr: 0.100000\n",
            "2020-10-22 16:17:26,399 epoch 1 - iter 24/128 - loss 23.34210511 - samples/sec: 15.75 - lr: 0.100000\n",
            "2020-10-22 16:17:36,685 epoch 1 - iter 36/128 - loss 20.00767240 - samples/sec: 18.67 - lr: 0.100000\n",
            "2020-10-22 16:17:50,365 epoch 1 - iter 48/128 - loss 19.07524232 - samples/sec: 14.04 - lr: 0.100000\n",
            "2020-10-22 16:18:03,084 epoch 1 - iter 60/128 - loss 18.24085093 - samples/sec: 15.10 - lr: 0.100000\n",
            "2020-10-22 16:18:15,198 epoch 1 - iter 72/128 - loss 17.34263970 - samples/sec: 15.85 - lr: 0.100000\n",
            "2020-10-22 16:18:28,192 epoch 1 - iter 84/128 - loss 16.82377655 - samples/sec: 14.78 - lr: 0.100000\n",
            "2020-10-22 16:18:40,375 epoch 1 - iter 96/128 - loss 16.29402604 - samples/sec: 15.76 - lr: 0.100000\n",
            "2020-10-22 16:18:52,421 epoch 1 - iter 108/128 - loss 15.66419473 - samples/sec: 15.95 - lr: 0.100000\n",
            "2020-10-22 16:19:05,881 epoch 1 - iter 120/128 - loss 15.00969523 - samples/sec: 14.27 - lr: 0.100000\n",
            "2020-10-22 16:19:13,518 ----------------------------------------------------------------------------------------------------\n",
            "2020-10-22 16:19:13,520 EPOCH 1 done: loss 14.6310 - lr 0.1000000\n",
            "2020-10-22 16:19:13,521 BAD EPOCHS (no improvement): 0\n",
            "2020-10-22 16:19:45,436 ----------------------------------------------------------------------------------------------------\n",
            "train mode resetting embeddings\n",
            "train mode resetting embeddings\n",
            "2020-10-22 16:19:49,023 epoch 2 - iter 12/128 - loss 10.14615631 - samples/sec: 54.98 - lr: 0.100000\n",
            "2020-10-22 16:19:53,017 epoch 2 - iter 24/128 - loss 9.61995625 - samples/sec: 48.10 - lr: 0.100000\n",
            "2020-10-22 16:19:59,170 epoch 2 - iter 36/128 - loss 9.07333083 - samples/sec: 46.34 - lr: 0.100000\n",
            "2020-10-22 16:20:03,530 epoch 2 - iter 48/128 - loss 9.95701118 - samples/sec: 44.06 - lr: 0.100000\n",
            "2020-10-22 16:20:07,502 epoch 2 - iter 60/128 - loss 9.69442242 - samples/sec: 48.38 - lr: 0.100000\n",
            "2020-10-22 16:20:11,318 epoch 2 - iter 72/128 - loss 9.44264598 - samples/sec: 50.34 - lr: 0.100000\n",
            "2020-10-22 16:20:16,191 epoch 2 - iter 84/128 - loss 9.08953623 - samples/sec: 39.42 - lr: 0.100000\n",
            "2020-10-22 16:20:20,964 epoch 2 - iter 96/128 - loss 9.05294951 - samples/sec: 40.25 - lr: 0.100000\n",
            "2020-10-22 16:20:25,456 epoch 2 - iter 108/128 - loss 9.02356106 - samples/sec: 42.76 - lr: 0.100000\n",
            "2020-10-22 16:20:28,782 epoch 2 - iter 120/128 - loss 8.95909836 - samples/sec: 57.77 - lr: 0.100000\n",
            "2020-10-22 16:20:31,474 ----------------------------------------------------------------------------------------------------\n",
            "2020-10-22 16:20:31,475 EPOCH 2 done: loss 8.7871 - lr 0.1000000\n",
            "2020-10-22 16:20:31,476 BAD EPOCHS (no improvement): 0\n",
            "2020-10-22 16:21:10,599 ----------------------------------------------------------------------------------------------------\n",
            "train mode resetting embeddings\n",
            "train mode resetting embeddings\n",
            "2020-10-22 16:21:15,205 epoch 3 - iter 12/128 - loss 6.56917769 - samples/sec: 42.07 - lr: 0.100000\n",
            "2020-10-22 16:21:18,921 epoch 3 - iter 24/128 - loss 7.90031406 - samples/sec: 51.69 - lr: 0.100000\n",
            "2020-10-22 16:21:23,348 epoch 3 - iter 36/128 - loss 7.35539836 - samples/sec: 43.40 - lr: 0.100000\n",
            "2020-10-22 16:21:27,904 epoch 3 - iter 48/128 - loss 7.64684950 - samples/sec: 42.16 - lr: 0.100000\n",
            "2020-10-22 16:21:32,458 epoch 3 - iter 60/128 - loss 7.64901282 - samples/sec: 42.18 - lr: 0.100000\n",
            "2020-10-22 16:21:36,996 epoch 3 - iter 72/128 - loss 7.55908090 - samples/sec: 42.33 - lr: 0.100000\n",
            "2020-10-22 16:21:41,224 epoch 3 - iter 84/128 - loss 7.33538686 - samples/sec: 45.46 - lr: 0.100000\n",
            "2020-10-22 16:21:44,910 epoch 3 - iter 96/128 - loss 7.20719671 - samples/sec: 52.12 - lr: 0.100000\n",
            "2020-10-22 16:21:48,895 epoch 3 - iter 108/128 - loss 7.00326729 - samples/sec: 48.22 - lr: 0.100000\n",
            "2020-10-22 16:21:52,767 epoch 3 - iter 120/128 - loss 6.95883865 - samples/sec: 49.62 - lr: 0.100000\n",
            "2020-10-22 16:21:55,112 ----------------------------------------------------------------------------------------------------\n",
            "2020-10-22 16:21:55,113 EPOCH 3 done: loss 6.9689 - lr 0.1000000\n",
            "2020-10-22 16:21:55,114 BAD EPOCHS (no improvement): 0\n",
            "2020-10-22 16:22:28,413 ----------------------------------------------------------------------------------------------------\n",
            "train mode resetting embeddings\n",
            "train mode resetting embeddings\n",
            "2020-10-22 16:22:32,397 epoch 4 - iter 12/128 - loss 7.41885336 - samples/sec: 48.62 - lr: 0.100000\n",
            "2020-10-22 16:22:36,640 epoch 4 - iter 24/128 - loss 7.05761346 - samples/sec: 45.28 - lr: 0.100000\n",
            "2020-10-22 16:22:41,084 epoch 4 - iter 36/128 - loss 6.94599791 - samples/sec: 43.23 - lr: 0.100000\n",
            "2020-10-22 16:22:45,644 epoch 4 - iter 48/128 - loss 6.69193043 - samples/sec: 42.13 - lr: 0.100000\n",
            "2020-10-22 16:22:49,920 epoch 4 - iter 60/128 - loss 6.47061285 - samples/sec: 44.92 - lr: 0.100000\n",
            "2020-10-22 16:22:54,273 epoch 4 - iter 72/128 - loss 6.37724673 - samples/sec: 44.14 - lr: 0.100000\n",
            "2020-10-22 16:22:59,255 epoch 4 - iter 84/128 - loss 6.79984692 - samples/sec: 38.55 - lr: 0.100000\n",
            "2020-10-22 16:23:03,283 epoch 4 - iter 96/128 - loss 6.73759996 - samples/sec: 47.70 - lr: 0.100000\n",
            "2020-10-22 16:23:07,776 epoch 4 - iter 108/128 - loss 6.69613828 - samples/sec: 42.76 - lr: 0.100000\n",
            "2020-10-22 16:23:11,429 epoch 4 - iter 120/128 - loss 6.51086526 - samples/sec: 52.58 - lr: 0.100000\n",
            "2020-10-22 16:23:13,527 ----------------------------------------------------------------------------------------------------\n",
            "2020-10-22 16:23:13,529 EPOCH 4 done: loss 6.3817 - lr 0.1000000\n",
            "2020-10-22 16:23:13,530 BAD EPOCHS (no improvement): 0\n",
            "2020-10-22 16:23:52,419 ----------------------------------------------------------------------------------------------------\n",
            "train mode resetting embeddings\n",
            "train mode resetting embeddings\n",
            "2020-10-22 16:23:57,000 epoch 5 - iter 12/128 - loss 5.50533392 - samples/sec: 46.17 - lr: 0.100000\n",
            "2020-10-22 16:24:01,839 epoch 5 - iter 24/128 - loss 5.61215523 - samples/sec: 39.70 - lr: 0.100000\n",
            "2020-10-22 16:24:05,726 epoch 5 - iter 36/128 - loss 5.33443630 - samples/sec: 49.43 - lr: 0.100000\n",
            "2020-10-22 16:24:09,903 epoch 5 - iter 48/128 - loss 5.28544476 - samples/sec: 45.99 - lr: 0.100000\n",
            "2020-10-22 16:24:14,414 epoch 5 - iter 60/128 - loss 5.39493242 - samples/sec: 42.59 - lr: 0.100000\n",
            "2020-10-22 16:24:18,349 epoch 5 - iter 72/128 - loss 5.19078582 - samples/sec: 48.82 - lr: 0.100000\n",
            "2020-10-22 16:24:22,272 epoch 5 - iter 84/128 - loss 5.20678050 - samples/sec: 48.97 - lr: 0.100000\n",
            "2020-10-22 16:24:26,222 epoch 5 - iter 96/128 - loss 5.14475928 - samples/sec: 48.64 - lr: 0.100000\n",
            "2020-10-22 16:24:30,866 epoch 5 - iter 108/128 - loss 5.82694705 - samples/sec: 41.37 - lr: 0.100000\n",
            "2020-10-22 16:24:34,497 epoch 5 - iter 120/128 - loss 5.74910715 - samples/sec: 52.90 - lr: 0.100000\n",
            "2020-10-22 16:24:36,568 ----------------------------------------------------------------------------------------------------\n",
            "2020-10-22 16:24:36,569 EPOCH 5 done: loss 5.6672 - lr 0.1000000\n",
            "2020-10-22 16:24:36,570 BAD EPOCHS (no improvement): 0\n",
            "2020-10-22 16:25:17,792 ----------------------------------------------------------------------------------------------------\n",
            "train mode resetting embeddings\n",
            "train mode resetting embeddings\n",
            "2020-10-22 16:25:21,737 epoch 6 - iter 12/128 - loss 5.04672867 - samples/sec: 49.25 - lr: 0.100000\n",
            "2020-10-22 16:25:26,264 epoch 6 - iter 24/128 - loss 4.92588648 - samples/sec: 42.44 - lr: 0.100000\n",
            "2020-10-22 16:25:30,797 epoch 6 - iter 36/128 - loss 4.85935679 - samples/sec: 42.38 - lr: 0.100000\n",
            "2020-10-22 16:25:35,714 epoch 6 - iter 48/128 - loss 5.88601901 - samples/sec: 39.07 - lr: 0.100000\n",
            "2020-10-22 16:25:40,767 epoch 6 - iter 60/128 - loss 5.71140058 - samples/sec: 38.02 - lr: 0.100000\n",
            "2020-10-22 16:25:45,125 epoch 6 - iter 72/128 - loss 5.61492979 - samples/sec: 44.08 - lr: 0.100000\n",
            "2020-10-22 16:25:48,497 epoch 6 - iter 84/128 - loss 5.34437429 - samples/sec: 56.99 - lr: 0.100000\n",
            "2020-10-22 16:25:51,905 epoch 6 - iter 96/128 - loss 5.19095127 - samples/sec: 56.37 - lr: 0.100000\n",
            "2020-10-22 16:25:56,124 epoch 6 - iter 108/128 - loss 5.12660445 - samples/sec: 45.53 - lr: 0.100000\n",
            "2020-10-22 16:25:59,839 epoch 6 - iter 120/128 - loss 5.06202198 - samples/sec: 51.71 - lr: 0.100000\n",
            "2020-10-22 16:26:02,172 ----------------------------------------------------------------------------------------------------\n",
            "2020-10-22 16:26:02,174 EPOCH 6 done: loss 5.0728 - lr 0.1000000\n",
            "2020-10-22 16:26:02,175 BAD EPOCHS (no improvement): 0\n",
            "2020-10-22 16:26:38,518 ----------------------------------------------------------------------------------------------------\n",
            "train mode resetting embeddings\n",
            "train mode resetting embeddings\n",
            "2020-10-22 16:26:42,140 epoch 7 - iter 12/128 - loss 4.68412193 - samples/sec: 53.57 - lr: 0.100000\n",
            "2020-10-22 16:26:45,878 epoch 7 - iter 24/128 - loss 4.39283793 - samples/sec: 51.39 - lr: 0.100000\n",
            "2020-10-22 16:26:50,737 epoch 7 - iter 36/128 - loss 4.23434590 - samples/sec: 39.54 - lr: 0.100000\n",
            "2020-10-22 16:26:54,870 epoch 7 - iter 48/128 - loss 4.46917045 - samples/sec: 46.48 - lr: 0.100000\n",
            "2020-10-22 16:26:58,894 epoch 7 - iter 60/128 - loss 4.36939507 - samples/sec: 47.73 - lr: 0.100000\n",
            "2020-10-22 16:27:03,796 epoch 7 - iter 72/128 - loss 4.60341435 - samples/sec: 39.19 - lr: 0.100000\n",
            "2020-10-22 16:27:08,621 epoch 7 - iter 84/128 - loss 5.11904077 - samples/sec: 39.81 - lr: 0.100000\n",
            "2020-10-22 16:27:12,620 epoch 7 - iter 96/128 - loss 5.00509749 - samples/sec: 48.06 - lr: 0.100000\n",
            "2020-10-22 16:27:16,542 epoch 7 - iter 108/128 - loss 4.98295298 - samples/sec: 48.98 - lr: 0.100000\n",
            "2020-10-22 16:27:20,076 epoch 7 - iter 120/128 - loss 4.86817951 - samples/sec: 54.36 - lr: 0.100000\n",
            "2020-10-22 16:27:22,646 ----------------------------------------------------------------------------------------------------\n",
            "2020-10-22 16:27:22,647 EPOCH 7 done: loss 4.8381 - lr 0.1000000\n",
            "2020-10-22 16:27:22,649 BAD EPOCHS (no improvement): 0\n",
            "2020-10-22 16:28:03,849 ----------------------------------------------------------------------------------------------------\n",
            "train mode resetting embeddings\n",
            "train mode resetting embeddings\n",
            "2020-10-22 16:28:08,176 epoch 8 - iter 12/128 - loss 3.91495969 - samples/sec: 44.79 - lr: 0.100000\n",
            "2020-10-22 16:28:12,223 epoch 8 - iter 24/128 - loss 3.89318808 - samples/sec: 47.47 - lr: 0.100000\n",
            "2020-10-22 16:28:16,137 epoch 8 - iter 36/128 - loss 4.10694306 - samples/sec: 49.08 - lr: 0.100000\n",
            "2020-10-22 16:28:20,614 epoch 8 - iter 48/128 - loss 5.51221066 - samples/sec: 42.91 - lr: 0.100000\n",
            "2020-10-22 16:28:25,229 epoch 8 - iter 60/128 - loss 5.22616856 - samples/sec: 41.62 - lr: 0.100000\n",
            "2020-10-22 16:28:29,695 epoch 8 - iter 72/128 - loss 5.10012174 - samples/sec: 43.02 - lr: 0.100000\n",
            "2020-10-22 16:28:33,981 epoch 8 - iter 84/128 - loss 4.99553045 - samples/sec: 44.85 - lr: 0.100000\n",
            "2020-10-22 16:28:37,372 epoch 8 - iter 96/128 - loss 4.82529812 - samples/sec: 56.65 - lr: 0.100000\n",
            "2020-10-22 16:28:41,890 epoch 8 - iter 108/128 - loss 4.83489876 - samples/sec: 42.51 - lr: 0.100000\n",
            "2020-10-22 16:28:45,994 epoch 8 - iter 120/128 - loss 4.75478545 - samples/sec: 46.82 - lr: 0.100000\n",
            "2020-10-22 16:28:48,093 ----------------------------------------------------------------------------------------------------\n",
            "2020-10-22 16:28:48,095 EPOCH 8 done: loss 4.7313 - lr 0.1000000\n",
            "2020-10-22 16:28:48,096 BAD EPOCHS (no improvement): 0\n",
            "2020-10-22 16:29:28,950 ----------------------------------------------------------------------------------------------------\n",
            "train mode resetting embeddings\n",
            "train mode resetting embeddings\n",
            "2020-10-22 16:29:32,315 epoch 9 - iter 12/128 - loss 3.48264865 - samples/sec: 57.61 - lr: 0.100000\n",
            "2020-10-22 16:29:36,581 epoch 9 - iter 24/128 - loss 3.64431064 - samples/sec: 45.03 - lr: 0.100000\n",
            "2020-10-22 16:29:40,150 epoch 9 - iter 36/128 - loss 3.48095578 - samples/sec: 53.84 - lr: 0.100000\n",
            "2020-10-22 16:29:44,428 epoch 9 - iter 48/128 - loss 3.57348108 - samples/sec: 44.90 - lr: 0.100000\n",
            "2020-10-22 16:29:49,374 epoch 9 - iter 60/128 - loss 4.35262831 - samples/sec: 38.83 - lr: 0.100000\n",
            "2020-10-22 16:29:53,830 epoch 9 - iter 72/128 - loss 4.31847001 - samples/sec: 43.11 - lr: 0.100000\n",
            "2020-10-22 16:29:57,805 epoch 9 - iter 84/128 - loss 4.28627331 - samples/sec: 48.33 - lr: 0.100000\n",
            "2020-10-22 16:30:01,982 epoch 9 - iter 96/128 - loss 4.24969990 - samples/sec: 45.99 - lr: 0.100000\n",
            "2020-10-22 16:30:06,068 epoch 9 - iter 108/128 - loss 4.13764910 - samples/sec: 47.02 - lr: 0.100000\n",
            "2020-10-22 16:30:09,992 epoch 9 - iter 120/128 - loss 4.15266607 - samples/sec: 48.96 - lr: 0.100000\n",
            "2020-10-22 16:30:12,633 ----------------------------------------------------------------------------------------------------\n",
            "2020-10-22 16:30:12,635 EPOCH 9 done: loss 4.1690 - lr 0.1000000\n",
            "2020-10-22 16:30:12,636 BAD EPOCHS (no improvement): 0\n",
            "2020-10-22 16:30:51,643 ----------------------------------------------------------------------------------------------------\n",
            "train mode resetting embeddings\n",
            "train mode resetting embeddings\n",
            "2020-10-22 16:30:55,174 epoch 10 - iter 12/128 - loss 3.39584160 - samples/sec: 54.90 - lr: 0.100000\n",
            "2020-10-22 16:30:59,357 epoch 10 - iter 24/128 - loss 6.72556559 - samples/sec: 45.93 - lr: 0.100000\n",
            "2020-10-22 16:31:03,168 epoch 10 - iter 36/128 - loss 5.58751887 - samples/sec: 50.41 - lr: 0.100000\n",
            "2020-10-22 16:31:08,107 epoch 10 - iter 48/128 - loss 5.18010410 - samples/sec: 38.90 - lr: 0.100000\n",
            "2020-10-22 16:31:12,425 epoch 10 - iter 60/128 - loss 4.88445969 - samples/sec: 44.48 - lr: 0.100000\n",
            "2020-10-22 16:31:16,591 epoch 10 - iter 72/128 - loss 4.75879643 - samples/sec: 46.11 - lr: 0.100000\n",
            "2020-10-22 16:31:20,782 epoch 10 - iter 84/128 - loss 4.52673845 - samples/sec: 45.85 - lr: 0.100000\n",
            "2020-10-22 16:31:25,436 epoch 10 - iter 96/128 - loss 4.46144697 - samples/sec: 41.29 - lr: 0.100000\n",
            "2020-10-22 16:31:29,733 epoch 10 - iter 108/128 - loss 4.44592824 - samples/sec: 44.72 - lr: 0.100000\n",
            "2020-10-22 16:31:33,891 epoch 10 - iter 120/128 - loss 4.43362142 - samples/sec: 46.19 - lr: 0.100000\n",
            "2020-10-22 16:31:36,344 ----------------------------------------------------------------------------------------------------\n",
            "2020-10-22 16:31:36,346 EPOCH 10 done: loss 4.3782 - lr 0.1000000\n",
            "2020-10-22 16:31:36,347 BAD EPOCHS (no improvement): 1\n",
            "2020-10-22 16:32:08,691 ----------------------------------------------------------------------------------------------------\n",
            "train mode resetting embeddings\n",
            "train mode resetting embeddings\n",
            "2020-10-22 16:32:21,204 epoch 11 - iter 12/128 - loss 3.66637087 - samples/sec: 50.16 - lr: 0.100000\n",
            "2020-10-22 16:32:25,125 epoch 11 - iter 24/128 - loss 3.69564904 - samples/sec: 48.99 - lr: 0.100000\n",
            "2020-10-22 16:32:29,534 epoch 11 - iter 36/128 - loss 3.74522063 - samples/sec: 43.57 - lr: 0.100000\n",
            "2020-10-22 16:32:33,475 epoch 11 - iter 48/128 - loss 3.62645971 - samples/sec: 48.74 - lr: 0.100000\n",
            "2020-10-22 16:32:37,781 epoch 11 - iter 60/128 - loss 3.51904422 - samples/sec: 44.61 - lr: 0.100000\n",
            "2020-10-22 16:32:42,157 epoch 11 - iter 72/128 - loss 3.54024844 - samples/sec: 43.90 - lr: 0.100000\n",
            "2020-10-22 16:32:47,013 epoch 11 - iter 84/128 - loss 3.53459781 - samples/sec: 39.56 - lr: 0.100000\n",
            "2020-10-22 16:32:51,780 epoch 11 - iter 96/128 - loss 3.57232660 - samples/sec: 40.30 - lr: 0.100000\n",
            "2020-10-22 16:32:55,689 epoch 11 - iter 108/128 - loss 3.53186648 - samples/sec: 49.14 - lr: 0.100000\n",
            "2020-10-22 16:32:59,629 epoch 11 - iter 120/128 - loss 3.51876225 - samples/sec: 48.76 - lr: 0.100000\n",
            "2020-10-22 16:33:02,574 ----------------------------------------------------------------------------------------------------\n",
            "2020-10-22 16:33:02,575 EPOCH 11 done: loss 4.0073 - lr 0.1000000\n",
            "2020-10-22 16:33:02,576 BAD EPOCHS (no improvement): 0\n",
            "2020-10-22 16:33:36,889 ----------------------------------------------------------------------------------------------------\n",
            "train mode resetting embeddings\n",
            "train mode resetting embeddings\n",
            "2020-10-22 16:33:41,698 epoch 12 - iter 12/128 - loss 3.58787760 - samples/sec: 40.20 - lr: 0.100000\n",
            "2020-10-22 16:33:45,328 epoch 12 - iter 24/128 - loss 3.73799900 - samples/sec: 52.93 - lr: 0.100000\n",
            "2020-10-22 16:33:49,621 epoch 12 - iter 36/128 - loss 3.57069591 - samples/sec: 44.75 - lr: 0.100000\n",
            "2020-10-22 16:33:54,210 epoch 12 - iter 48/128 - loss 3.78556142 - samples/sec: 41.85 - lr: 0.100000\n",
            "2020-10-22 16:33:58,351 epoch 12 - iter 60/128 - loss 3.74417367 - samples/sec: 46.40 - lr: 0.100000\n",
            "2020-10-22 16:34:02,767 epoch 12 - iter 72/128 - loss 3.67752771 - samples/sec: 43.51 - lr: 0.100000\n",
            "2020-10-22 16:34:06,851 epoch 12 - iter 84/128 - loss 3.62446581 - samples/sec: 47.05 - lr: 0.100000\n",
            "2020-10-22 16:34:11,165 epoch 12 - iter 96/128 - loss 3.68773111 - samples/sec: 44.54 - lr: 0.100000\n",
            "2020-10-22 16:34:15,269 epoch 12 - iter 108/128 - loss 3.67235264 - samples/sec: 46.80 - lr: 0.100000\n",
            "2020-10-22 16:34:19,508 epoch 12 - iter 120/128 - loss 3.98715760 - samples/sec: 45.31 - lr: 0.100000\n",
            "2020-10-22 16:34:21,776 ----------------------------------------------------------------------------------------------------\n",
            "2020-10-22 16:34:21,777 EPOCH 12 done: loss 3.9203 - lr 0.1000000\n",
            "2020-10-22 16:34:21,779 BAD EPOCHS (no improvement): 0\n",
            "2020-10-22 16:34:55,693 ----------------------------------------------------------------------------------------------------\n",
            "train mode resetting embeddings\n",
            "train mode resetting embeddings\n",
            "2020-10-22 16:34:59,015 epoch 13 - iter 12/128 - loss 3.33703688 - samples/sec: 58.39 - lr: 0.100000\n",
            "2020-10-22 16:35:02,822 epoch 13 - iter 24/128 - loss 3.09133979 - samples/sec: 50.45 - lr: 0.100000\n",
            "2020-10-22 16:35:07,125 epoch 13 - iter 36/128 - loss 3.24480231 - samples/sec: 44.65 - lr: 0.100000\n",
            "2020-10-22 16:35:11,324 epoch 13 - iter 48/128 - loss 3.33183773 - samples/sec: 45.75 - lr: 0.100000\n",
            "2020-10-22 16:35:15,269 epoch 13 - iter 60/128 - loss 3.28117129 - samples/sec: 48.69 - lr: 0.100000\n",
            "2020-10-22 16:35:20,244 epoch 13 - iter 72/128 - loss 3.33937876 - samples/sec: 38.62 - lr: 0.100000\n",
            "2020-10-22 16:35:24,514 epoch 13 - iter 84/128 - loss 3.35890032 - samples/sec: 44.99 - lr: 0.100000\n",
            "2020-10-22 16:35:28,327 epoch 13 - iter 96/128 - loss 3.33795651 - samples/sec: 50.38 - lr: 0.100000\n",
            "2020-10-22 16:35:33,032 epoch 13 - iter 108/128 - loss 3.96878277 - samples/sec: 40.83 - lr: 0.100000\n",
            "2020-10-22 16:35:37,038 epoch 13 - iter 120/128 - loss 3.91841328 - samples/sec: 47.95 - lr: 0.100000\n",
            "2020-10-22 16:35:39,577 ----------------------------------------------------------------------------------------------------\n",
            "2020-10-22 16:35:39,578 EPOCH 13 done: loss 3.9792 - lr 0.1000000\n",
            "2020-10-22 16:35:39,580 BAD EPOCHS (no improvement): 1\n",
            "2020-10-22 16:36:18,484 ----------------------------------------------------------------------------------------------------\n",
            "train mode resetting embeddings\n",
            "train mode resetting embeddings\n",
            "2020-10-22 16:36:22,951 epoch 14 - iter 12/128 - loss 2.84165444 - samples/sec: 43.31 - lr: 0.100000\n",
            "2020-10-22 16:36:27,095 epoch 14 - iter 24/128 - loss 2.99561187 - samples/sec: 46.36 - lr: 0.100000\n",
            "2020-10-22 16:36:31,195 epoch 14 - iter 36/128 - loss 2.82841931 - samples/sec: 46.86 - lr: 0.100000\n",
            "2020-10-22 16:36:35,653 epoch 14 - iter 48/128 - loss 3.09042556 - samples/sec: 43.10 - lr: 0.100000\n",
            "2020-10-22 16:36:39,878 epoch 14 - iter 60/128 - loss 3.14244358 - samples/sec: 45.46 - lr: 0.100000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOlVhB0Xq7fJ"
      },
      "source": [
        "from flair.data import Corpus\n",
        "from flair.datasets import CONLL_03\n",
        "from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings, PooledFlairEmbeddings\n",
        "from typing import List\n",
        "from flair.data import Dictionary\n",
        "from flair.models import LanguageModel\n",
        "from flair.trainers.language_model_trainer import LanguageModelTrainer, TextCorpus\n",
        "from flair.data import Corpus\n",
        "from flair.datasets import ColumnCorpus\n",
        "\n",
        "# define columns\n",
        "columns = {0: 'text', 1: 'ner'}\n",
        "\n",
        "# this is the folder in which train, test and dev files reside\n",
        "data_folder = '/content/gdrive/My Drive/resources/tasks/conll_03'\n",
        "#data_folder = '/content/gdrive/My Drive/conll_03'\n",
        "\n",
        "# init a corpus using column format, data folder and the names of the train, dev and test files\n",
        "corpus: Corpus = ColumnCorpus(data_folder, columns,\n",
        "                              train_file='train.txt',\n",
        "                              test_file='test.txt',\n",
        "                              dev_file='dev.txt')\n",
        "    \n",
        "\n",
        "\n",
        "# initialize trainer\n",
        "from flair.trainers import ModelTrainer\n",
        "\n",
        "checkpoint = '/content/gdrive/My Drive/resources/taggers/example-ner/checkpoint.pt'\n",
        "from pathlib import Path\n",
        "from torch.optim.adam import Adam\n",
        "trainer = ModelTrainer.load_checkpoint(checkpoint, corpus)\n",
        "\n",
        "trainer.train('/content/gdrive/My Drive/resources/taggers/example-ner',\n",
        "              train_with_dev=True, \n",
        "              learning_rate=0.1,   \n",
        "              mini_batch_size=16,\n",
        "              max_epochs=150,\n",
        "              checkpoint=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aozUj1PR1Mwc"
      },
      "source": [
        "\n",
        "from flair.models import SequenceTagger\n",
        "model = SequenceTagger.load('/content/gdrive/My Drive/resources/taggers/example-ner/final-model.pt')\n",
        "from flair.data import Sentence\n",
        "# create example sentence\n",
        "sentence = Sentence('إياد ومريم يلعبون فى الحديقة') \n",
        "\n",
        "# predict tags and print\n",
        "model.predict(sentence)\n",
        "#model.evaluate()\n",
        "print(sentence.to_tagged_string('ner'))\n",
        "\n",
        "for token in sentence:\n",
        "    tag = token.get_tag('ner')\n",
        "    print(f'\"{token}\" is tagged as \"{tag.value}\" with confidence score \"{tag.score}\"')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}